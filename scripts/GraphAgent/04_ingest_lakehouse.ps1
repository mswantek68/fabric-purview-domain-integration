#!/usr/bin/env pwsh

<#
.SYNOPSIS
    Ingest document summaries into Fabric lakehouse and expose via Data Agent

.DESCRIPTION
    This script takes document summaries generated by the summarization process
    and ingests them into a Fabric lakehouse. The summaries are structured for
    easy querying and consumption by Fabric Data Agents.

.PARAMETER InputFile
    Path to JSON file containing document summaries

.PARAMETER LakehouseName
    Name of the Fabric lakehouse to ingest data into

.PARAMETER WorkspaceName
    Name of the Fabric workspace containing the lakehouse

.PARAMETER TableName
    Name of the table to create/update (default: document_summaries)

.PARAMETER PartitionBy
    Column to partition by (default: search_term)

.PARAMETER UpdateMode
    Update mode: 'append', 'overwrite', or 'merge' (default: append)

.EXAMPLE
    ./04_ingest_summaries.ps1 -InputFile "document_summaries.json" -LakehouseName "bronze"
    
.EXAMPLE
    ./04_ingest_summaries.ps1 -InputFile "document_summaries.json" -UpdateMode "overwrite"
#>

[CmdletBinding()]
param(
    [Parameter(Mandatory = $false)]
    [string]$InputFile = "",
    
    [Parameter(Mandatory = $false)]
    [string]$LakehouseName = "",
    
    [Parameter(Mandatory = $false)]
    [string]$WorkspaceName = "",
    
    [Parameter(Mandatory = $false)]
    [string]$TableName = "document_summaries",
    
    [Parameter(Mandatory = $false)]
    [string]$PartitionBy = "search_term",
    
    [Parameter(Mandatory = $false)]
    [ValidateSet('append', 'overwrite', 'merge')]
    [string]$UpdateMode = "append"
)

Set-StrictMode -Version Latest
$ErrorActionPreference = "Stop"

# Logging functions
function Log([string]$message) { 
    Write-Host "[lakehouse-ingest] $message" -ForegroundColor Cyan 
}

function Success([string]$message) { 
    Write-Host "[lakehouse-ingest] ✅ $message" -ForegroundColor Green 
}

function Warn([string]$message) { 
    Write-Warning "[lakehouse-ingest] ⚠️ $message" 
}

function Error([string]$message) { 
    Write-Error "[lakehouse-ingest] ❌ $message" 
}

# Get configuration from azd environment
function Get-FabricConfiguration {
    Log "Getting Fabric configuration..."
    
    $config = @{
        LakehouseName = $LakehouseName
        WorkspaceName = $WorkspaceName
    }
    
    # Try to get from azd environment if not provided
    if (-not $config.LakehouseName -or -not $config.WorkspaceName) {
        try {
            $azdEnvValues = azd env get-values 2>$null
            if ($azdEnvValues) {
                $env_vars = @{}
                foreach ($line in $azdEnvValues) {
                    if ($line -match '^(.+?)=(.*)$') {
                        $env_vars[$matches[1]] = $matches[2].Trim('"')
                    }
                }
                
                if (-not $config.LakehouseName) { 
                    $config.LakehouseName = $env_vars['documentLakehouseName'] ?? 'bronze'
                }
                if (-not $config.WorkspaceName) { 
                    $config.WorkspaceName = $env_vars['desiredFabricWorkspaceName'] ?? 'ws001'
                }
            }
        }
        catch {
            Warn "Could not read azd environment"
        }
    }
    
    if (-not $config.LakehouseName) {
        throw "Lakehouse name is required"
    }
    if (-not $config.WorkspaceName) {
        throw "Workspace name is required"
    }
    
    Log "Target lakehouse: $($config.LakehouseName)"
    Log "Target workspace: $($config.WorkspaceName)"
    
    return $config
}

# Get Fabric workspace and lakehouse details
function Get-FabricWorkspaceInfo {
    param(
        [string]$WorkspaceName
    )
    
    Log "Getting Fabric workspace information..."
    
    try {
        # Get Power BI access token for Fabric APIs
        $fabricToken = az account get-access-token --resource https://analysis.windows.net/powerbi/api --query accessToken -o tsv
        $headers = @{
            'Authorization' = "Bearer $fabricToken"
            'Content-Type' = 'application/json'
        }
        
        # Get workspace by name
        $workspacesUri = "https://api.powerbi.com/v1.0/myorg/groups"
        $workspaces = Invoke-RestMethod -Uri $workspacesUri -Headers $headers -Method Get
        
        $workspace = $workspaces.value | Where-Object { $_.name -eq $WorkspaceName }
        
        if (-not $workspace) {
            throw "Workspace '$WorkspaceName' not found"
        }
        
        Log "Found workspace: $($workspace.name) (ID: $($workspace.id))"
        return @{
            Id = $workspace.id
            Name = $workspace.name
            Headers = $headers
        }
    }
    catch {
        Error "Failed to get workspace information: $_"
        throw
    }
}

# Get lakehouse information from workspace
function Get-LakehouseInfo {
    param(
        [hashtable]$Workspace,
        [string]$LakehouseName
    )
    
    Log "Getting lakehouse information..."
    
    try {
        # Get lakehouses in workspace using Fabric API
        $lakehousesUri = "https://api.fabric.microsoft.com/v1/workspaces/$($Workspace.Id)/lakehouses"
        $lakehouses = Invoke-RestMethod -Uri $lakehousesUri -Headers $Workspace.Headers -Method Get
        
        $lakehouse = $lakehouses.value | Where-Object { $_.displayName -eq $LakehouseName }
        
        if (-not $lakehouse) {
            throw "Lakehouse '$LakehouseName' not found in workspace '$($Workspace.Name)'"
        }
        
        Log "Found lakehouse: $($lakehouse.displayName) (ID: $($lakehouse.id))"
        return @{
            Id = $lakehouse.id
            Name = $lakehouse.displayName
            WorkspaceId = $Workspace.Id
            OneLakeDataAccessEndpoint = $lakehouse.properties.oneLakeDataAccessEndpoint
        }
    }
    catch {
        Error "Failed to get lakehouse information: $_"
        throw
    }
}

# Load document summaries from input file
function Get-DocumentSummaries {
    param([string]$FilePath)
    
    if ([string]::IsNullOrEmpty($FilePath)) {
        # Find latest summaries file
        $summaryFiles = Get-ChildItem -Path "." -Name "document_summaries_*.json" | Sort-Object LastWriteTime -Descending
        if ($summaryFiles.Count -gt 0) {
            $FilePath = $summaryFiles[0].FullName
            Log "Using latest summaries file: $($summaryFiles[0].Name)"
        } else {
            throw "No input file specified and no summaries files found"
        }
    }
    
    if (-not (Test-Path $FilePath)) {
        throw "Input file not found: $FilePath"
    }
    
    Log "Loading document summaries from: $FilePath"
    $content = Get-Content -Path $FilePath -Raw | ConvertFrom-Json
    
    if (-not $content.Summaries) {
        throw "No summaries found in input file"
    }
    
    return $content
}

# Transform summaries to structured table format
function ConvertTo-TableData {
    param(
        [array]$Summaries,
        [hashtable]$Metadata
    )
    
    Log "Transforming summaries to table format..."
    
    $tableData = @()
    
    foreach ($summary in $Summaries) {
        # Skip failed summaries
        if ($summary.Error) {
            Warn "Skipping failed summary for: $($summary.DocumentName)"
            continue
        }
        
        # Parse summary content to extract structured fields
        $structuredData = Parse-SummaryContent -Summary $summary.Summary
        
        # Create table row
        $row = @{
            # Primary identifiers
            document_id = $summary.DocumentId ?? [System.Guid]::NewGuid().ToString()
            document_name = $summary.DocumentName
            source = $summary.Source
            file_type = $summary.FileType
            
            # Metadata
            last_modified = $summary.LastModified
            search_term = $summary.SearchTerm
            web_url = $summary.WebUrl
            generated_at = $summary.GeneratedAt
            model_used = $summary.ModelUsed
            
            # Structured summary fields
            title = $structuredData.Title
            document_type = $structuredData.DocumentType
            key_topics = $structuredData.KeyTopics
            organization_references = $structuredData.OrganizationReferences
            key_decisions = $structuredData.KeyDecisions
            action_items = $structuredData.ActionItems
            key_people = $structuredData.KeyPeople
            important_dates = $structuredData.ImportantDates
            relevance_reason = $structuredData.RelevanceReason
            executive_summary = $structuredData.ExecutiveSummary
            tags = $structuredData.Tags
            
            # Full summary text
            full_summary = $summary.Summary
            
            # Ingestion metadata
            ingestion_timestamp = (Get-Date).ToString("yyyy-MM-ddTHH:mm:ssZ")
            ingestion_batch_id = $Metadata.BatchId
            source_file = $Metadata.SourceFile
        }
        
        $tableData += $row
    }
    
    Log "Transformed $($tableData.Count) summaries to table format"
    return $tableData
}

# Parse summary content to extract structured fields
function Parse-SummaryContent {
    param([string]$Summary)
    
    $structuredData = @{
        Title = ""
        DocumentType = ""
        KeyTopics = ""
        OrganizationReferences = ""
        KeyDecisions = ""
        ActionItems = ""
        KeyPeople = ""
        ImportantDates = ""
        RelevanceReason = ""
        ExecutiveSummary = ""
        Tags = ""
    }
    
    # Extract fields using regex patterns
    $patterns = @{
        Title = '\*\*Title\*\*:\s*([^\n\r]+)'
        DocumentType = '\*\*Document Type\*\*:\s*([^\n\r]+)'
        KeyTopics = '\*\*Key Topics\*\*:\s*([^\n\r]+)'
        OrganizationReferences = '\*\*Organization References\*\*:\s*([^\n\r]+)'
        KeyDecisions = '\*\*Key Decisions\*\*:\s*([^\n\r]+)'
        ActionItems = '\*\*Action Items\*\*:\s*([^\n\r]+)'
        KeyPeople = '\*\*Key People\*\*:\s*([^\n\r]+)'
        ImportantDates = '\*\*Dates\*\*:\s*([^\n\r]+)'
        RelevanceReason = '\*\*Relevance\*\*:\s*([^\n\r]+)'
        Tags = '\*\*Tags\*\*:\s*([^\n\r]+)'
    }
    
    foreach ($field in $patterns.Keys) {
        if ($Summary -match $patterns[$field]) {
            $structuredData[$field] = $matches[1].Trim()
        }
    }
    
    # Extract executive summary (multi-line)
    if ($Summary -match '\*\*Executive Summary\*\*[^\n\r]*:\s*\n([^*]+)') {
        $structuredData.ExecutiveSummary = $matches[1].Trim()
    }
    
    return $structuredData
}

# Create or update lakehouse table
function Set-LakehouseTable {
    param(
        [hashtable]$Lakehouse,
        [array]$TableData,
        [hashtable]$Workspace,
        [string]$TableName,
        [string]$UpdateMode
    )
    
    Log "Creating/updating lakehouse table: $TableName"
    
    try {
        # Convert table data to Delta format for Fabric
        $deltaData = $TableData | ConvertTo-Json -Depth 3
        
        # Create temporary file for upload
        $tempFile = [System.IO.Path]::GetTempFileName()
        $deltaData | Out-File -FilePath $tempFile -Encoding UTF8
        
        # Upload data to lakehouse using Fabric API
        $uploadUri = "https://api.fabric.microsoft.com/v1/workspaces/$($Workspace.Id)/lakehouses/$($Lakehouse.Id)/tables/$TableName/load"
        
        $uploadBody = @{
            relativePath = "Tables/$TableName"
            pathType = "File"
            fileFormat = @{
                type = "Json"
            }
            loadOptions = @{
                mode = $UpdateMode
            }
        }
        
        # Note: This is a simplified approach. In practice, you would:
        # 1. Upload files to OneLake storage endpoint
        # 2. Use Fabric notebooks or pipelines for complex transformations
        # 3. Use Delta Lake format for optimal performance
        
        Log "Uploading $($TableData.Count) records to table '$TableName'"
        
        # For now, save as files that can be manually ingested or processed by notebooks
        $outputDir = "lakehouse_data"
        if (-not (Test-Path $outputDir)) {
            New-Item -ItemType Directory -Path $outputDir | Out-Null
        }
        
        # Save as multiple formats for flexibility
        $timestamp = Get-Date -Format 'yyyyMMdd_HHmmss'
        
        # JSON format
        $jsonFile = Join-Path $outputDir "$($TableName)_$timestamp.json"
        $TableData | ConvertTo-Json -Depth 3 | Out-File -FilePath $jsonFile -Encoding UTF8
        
        # CSV format
        $csvFile = Join-Path $outputDir "$($TableName)_$timestamp.csv"
        $TableData | Export-Csv -Path $csvFile -NoTypeInformation
        
        # Parquet format would be ideal but requires additional tools
        
        Success "Data prepared for lakehouse ingestion:"
        Log "  • JSON: $jsonFile"
        Log "  • CSV: $csvFile"
        
        # Clean up temp file
        Remove-Item $tempFile -Force
        
        return @{
            JsonFile = $jsonFile
            CsvFile = $csvFile
            RecordCount = $TableData.Count
            TableName = $TableName
        }
    }
    catch {
        Error "Failed to create/update lakehouse table: $_"
        throw
    }
}

# Create Fabric notebook for data processing
function New-FabricNotebook {
    param(
        [hashtable]$Lakehouse,
        [hashtable]$Workspace,
        [string]$TableName,
        [array]$DataFiles
    )
    
    Log "Creating Fabric notebook for data processing..."
    
    # Create notebook content
    $notebookContent = @"
# Document Summaries Data Processing Notebook

This notebook processes document summaries and loads them into the lakehouse.

## Configuration

```python
import pandas as pd
from pyspark.sql import SparkSession
from delta.tables import DeltaTable
import json

# Initialize Spark session
spark = SparkSession.builder.appName("DocumentSummariesProcessing").getOrCreate()

# Lakehouse configuration
lakehouse_name = "$($Lakehouse.Name)"
table_name = "$TableName"
workspace_id = "$($Workspace.Id)"
```

## Load Data

```python
# Load JSON data
json_file = "Files/lakehouse_data/$($TableName)_*.json"
df = spark.read.option("multiline", "true").json(json_file)

# Display schema and sample data
df.printSchema()
df.show(5, truncate=False)
```

## Data Transformation

```python
# Clean and transform data
from pyspark.sql.functions import col, current_timestamp, lit
from pyspark.sql.types import TimestampType

# Convert timestamp strings to proper timestamp type
df_transformed = df.withColumn("last_modified_ts", col("last_modified").cast(TimestampType())) \
                   .withColumn("generated_at_ts", col("generated_at").cast(TimestampType())) \
                   .withColumn("ingestion_timestamp_ts", col("ingestion_timestamp").cast(TimestampType())) \
                   .withColumn("processed_at", current_timestamp())

# Add partitioning columns
df_partitioned = df_transformed.withColumn("year", year(col("last_modified_ts"))) \
                               .withColumn("month", month(col("last_modified_ts"))) \
                               .withColumn("search_term_clean", regexp_replace(col("search_term"), "[^a-zA-Z0-9]", "_"))
```

## Write to Delta Table

```python
# Write to Delta table with partitioning
delta_table_path = f"Tables/{table_name}"

df_partitioned.write \
    .format("delta") \
    .mode("append") \
    .partitionBy("search_term_clean", "year", "month") \
    .option("mergeSchema", "true") \
    .saveAsTable(table_name)

print(f"Successfully wrote {df_partitioned.count()} records to {table_name}")
```

## Create Views and Summaries

```python
# Create view for easy querying
df_partitioned.createOrReplaceTempView("document_summaries_view")

# Create summary statistics
summary_stats = spark.sql("""
    SELECT 
        source,
        search_term,
        COUNT(*) as document_count,
        COUNT(DISTINCT file_type) as unique_file_types,
        MAX(last_modified_ts) as latest_document,
        COUNT(DISTINCT organization_references) as unique_organizations
    FROM document_summaries_view 
    GROUP BY source, search_term
    ORDER BY document_count DESC
""")

summary_stats.show()
summary_stats.write.mode("overwrite").saveAsTable("document_summaries_stats")
```

## Data Quality Checks

```python
# Check for missing required fields
quality_check = spark.sql("""
    SELECT 
        COUNT(*) as total_records,
        COUNT(CASE WHEN document_name IS NULL OR document_name = '' THEN 1 END) as missing_names,
        COUNT(CASE WHEN source IS NULL OR source = '' THEN 1 END) as missing_sources,
        COUNT(CASE WHEN full_summary IS NULL OR full_summary = '' THEN 1 END) as missing_summaries,
        COUNT(CASE WHEN search_term IS NULL OR search_term = '' THEN 1 END) as missing_search_terms
    FROM document_summaries_view
""")

quality_check.show()
```

## Export Summary Report

```python
# Generate summary report
report_data = spark.sql("""
    SELECT 
        'Documents Processed' as metric,
        COUNT(*) as value
    FROM document_summaries_view
    
    UNION ALL
    
    SELECT 
        'Unique Sources' as metric,
        COUNT(DISTINCT source) as value
    FROM document_summaries_view
    
    UNION ALL
    
    SELECT 
        'Search Terms' as metric,
        COUNT(DISTINCT search_term) as value
    FROM document_summaries_view
    
    UNION ALL
    
    SELECT 
        'Organizations Mentioned' as metric,
        COUNT(DISTINCT organization_references) as value
    FROM document_summaries_view
    WHERE organization_references IS NOT NULL AND organization_references != ''
""")

report_data.show()

# Save report
report_data.write.mode("overwrite").option("header", "true").csv("Files/reports/processing_summary")
```
"@

    # Save notebook content
    $notebookDir = "fabric_notebooks"
    if (-not (Test-Path $notebookDir)) {
        New-Item -ItemType Directory -Path $notebookDir | Out-Null
    }
    
    $notebookFile = Join-Path $notebookDir "document_summaries_processing.py"
    $notebookContent | Out-File -FilePath $notebookFile -Encoding UTF8
    
    Success "Created Fabric notebook: $notebookFile"
    return $notebookFile
}

# Main execution
try {
    Log "=============================================================="
    Log "Ingesting document summaries into Fabric lakehouse"
    Log "=============================================================="
    
    # Get configuration
    $fabricConfig = Get-FabricConfiguration
    
    # Load document summaries
    $summariesData = Get-DocumentSummaries -FilePath $InputFile
    Log "Loaded $($summariesData.Summaries.Count) summaries"
    
    # Get Fabric workspace and lakehouse information
    $workspace = Get-FabricWorkspaceInfo -WorkspaceName $fabricConfig.WorkspaceName
    $lakehouse = Get-LakehouseInfo -Workspace $workspace -LakehouseName $fabricConfig.LakehouseName
    
    # Transform summaries to table format
    $batchId = [System.Guid]::NewGuid().ToString()
    $metadata = @{
        BatchId = $batchId
        SourceFile = $InputFile
        Timestamp = (Get-Date).ToString("yyyy-MM-ddTHH:mm:ssZ")
    }
    
    $tableData = ConvertTo-TableData -Summaries $summariesData.Summaries -Metadata $metadata
    
    if ($tableData.Count -eq 0) {
        Warn "No valid summaries found to ingest"
        exit 0
    }
    
    # Create/update lakehouse table
    $ingestionResult = Set-LakehouseTable -Lakehouse $lakehouse -TableData $tableData -Workspace $workspace -TableName $TableName -UpdateMode $UpdateMode
    
    # Create Fabric notebook for processing
    $notebookFile = New-FabricNotebook -Lakehouse $lakehouse -Workspace $workspace -TableName $TableName -DataFiles @($ingestionResult.JsonFile, $ingestionResult.CsvFile)
    
    # Create ingestion report
    $report = @{
        Timestamp = (Get-Date).ToString("yyyy-MM-ddTHH:mm:ssZ")
        BatchId = $batchId
        Configuration = @{
            InputFile = $InputFile
            LakehouseName = $fabricConfig.LakehouseName
            WorkspaceName = $fabricConfig.WorkspaceName
            TableName = $TableName
            UpdateMode = $UpdateMode
        }
        Statistics = @{
            SourceSummaries = $summariesData.Summaries.Count
            ValidSummaries = $tableData.Count
            SkippedSummaries = $summariesData.Summaries.Count - $tableData.Count
            UniqueSearchTerms = ($tableData | Select-Object -ExpandProperty search_term -Unique).Count
            UniqueSources = ($tableData | Select-Object -ExpandProperty source -Unique).Count
            UniqueFileTypes = ($tableData | Select-Object -ExpandProperty file_type -Unique).Count
        }
        Files = @{
            JsonData = $ingestionResult.JsonFile
            CsvData = $ingestionResult.CsvFile
            NotebookFile = $notebookFile
        }
        Lakehouse = @{
            WorkspaceId = $workspace.Id
            LakehouseId = $lakehouse.Id
            TableName = $TableName
        }
    }
    
    # Save ingestion report
    $reportFile = "ingestion_report_$(Get-Date -Format 'yyyyMMdd_HHmmss').json"
    $report | ConvertTo-Json -Depth 5 | Out-File -FilePath $reportFile -Encoding UTF8
    
    # Display results
    Log ""
    Success "Lakehouse ingestion completed!"
    Log "📊 Ingestion Statistics:"
    Log "  • Source summaries: $($report.Statistics.SourceSummaries)"
    Log "  • Valid summaries ingested: $($report.Statistics.ValidSummaries)"
    Log "  • Skipped summaries: $($report.Statistics.SkippedSummaries)"
    Log "  • Unique search terms: $($report.Statistics.UniqueSearchTerms)"
    Log "  • Unique sources: $($report.Statistics.UniqueSources)"
    Log "  • Unique file types: $($report.Statistics.UniqueFileTypes)"
    
    Log ""
    Log "📁 Generated Files:"
    Log "  • Data (JSON): $($ingestionResult.JsonFile)"
    Log "  • Data (CSV): $($ingestionResult.CsvFile)"
    Log "  • Processing notebook: $notebookFile"
    Log "  • Ingestion report: $reportFile"
    
    Log ""
    Success "Next Steps:"
    Log "1. Upload data files to the lakehouse Files section"
    Log "2. Import and run the processing notebook in Fabric"
    Log "3. Verify data in the '$TableName' table"
    Log "4. Set up Data Agent connections for querying"
    
    Log ""
    Success "✅ Lakehouse ingestion preparation completed successfully!"
    
} catch {
    Error "Lakehouse ingestion failed: $_"
    exit 1
}