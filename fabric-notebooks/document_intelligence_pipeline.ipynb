{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "# Fabric Document Intelligence Pipeline\n",
        "This notebook orchestrates Azure AI Document Intelligence against documents stored in a Fabric lakehouse. It is designed to be executed from a Fabric Data Factory (Data Pipeline) event trigger when new files land in the document folders.\n",
        "\n",
        "**Steps performed:**\n",
        "1. Resolve pipeline parameters (Document Intelligence endpoint, lakehouse name, document types).\n",
        "2. Discover new documents under `Files/documents/<type>/` within the lakehouse.\n",
        "3. Invoke the Azure AI Document Intelligence REST API for each document.\n",
        "4. Persist normalized JSON outputs back to `Files/raw/document-intelligence/...` and emit manifests.\n",
        "5. Materialize silver Delta tables (`silver_invoice_*`, `silver_utility_bill_*`).\n",
        "\n",
        "Configure your pipeline trigger to supply the required parameters (see repository README for details).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "from notebookutils import mssparkutils\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "def get_parameter(name: str, default: Optional[str] = None) -> Optional[str]:\n",
        "    \"\"\"Resolve a pipeline parameter, falling back to environment variables.\"\"\"\n",
        "    try:\n",
        "        value = mssparkutils.env.getJobParameter(name)\n",
        "        if value is not None and str(value).strip() and str(value).lower() != \"none\":\n",
        "            return value\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    env_value = os.getenv(name)\n",
        "    if env_value is None:\n",
        "        env_value = os.getenv(name.upper())\n",
        "    if env_value is not None and str(env_value).strip() and str(env_value).lower() != \"none\":\n",
        "        return env_value\n",
        "\n",
        "    return default\n",
        "\n",
        "\n",
        "document_types_raw = get_parameter(\"documentTypes\", \"invoice,utility-bill\")\n",
        "document_types = [token.strip().lower() for token in document_types_raw.split(',') if token.strip()]\n",
        "\n",
        "config: Dict[str, Any] = {\n",
        "    \"endpoint\": get_parameter(\"documentIntelligenceEndpoint\", os.getenv(\"DOCUMENT_INTELLIGENCE_ENDPOINT\", \"\")),\n",
        "    \"api_version\": get_parameter(\"documentIntelligenceApiVersion\", \"2023-07-31\"),\n",
        "    \"lakehouse\": get_parameter(\"documentLakehouseName\", os.getenv(\"DOCUMENT_LAKEHOUSE_NAME\", \"\")),\n",
        "    \"workspace_id\": get_parameter(\"workspaceId\", os.getenv(\"FABRIC_WORKSPACE_ID\", \"\")),\n",
        "    \"max_documents\": int(get_parameter(\"maxDocumentsPerType\", \"25\")),\n",
        "    \"force_reprocess\": get_parameter(\"forceReprocess\", \"false\").lower() == \"true\"\n",
        "}\n",
        "\n",
        "if not config[\"endpoint\"]:\n",
        "    raise ValueError(\"Document Intelligence endpoint is required. Provide pipeline parameter 'documentIntelligenceEndpoint'.\")\n",
        "if not config[\"lakehouse\"]:\n",
        "    raise ValueError(\"Lakehouse name is required. Provide pipeline parameter 'documentLakehouseName'.\")\n",
        "\n",
        "try:\n",
        "    spark.sql(f\"USE {config['lakehouse']}\")\n",
        "except Exception:\n",
        "    # If the lakehouse database has not been created yet, this will no-op; write operations will create it lazily.\n",
        "    pass\n",
        "\n",
        "_DOC_TYPE_DEFAULTS: Dict[str, Dict[str, Any]] = {\n",
        "    \"invoice\": {\n",
        "        \"source_subfolder\": \"invoices\",\n",
        "        \"parameter_name\": \"invoiceModelId\",\n",
        "        \"header_table\": \"silver_invoice_header\",\n",
        "        \"line_table\": \"silver_invoice_line\"\n",
        "    },\n",
        "    \"utility-bill\": {\n",
        "        \"source_subfolder\": \"utility-bills\",\n",
        "        \"parameter_name\": \"utilityModelId\",\n",
        "        \"header_table\": \"silver_utility_bill_header\",\n",
        "        \"line_table\": \"silver_utility_bill_line\"\n",
        "    }\n",
        "}\n",
        "\n",
        "doc_type_configs: Dict[str, Dict[str, Any]] = {}\n",
        "for doc_type in document_types:\n",
        "    defaults = _DOC_TYPE_DEFAULTS.get(doc_type, _DOC_TYPE_DEFAULTS[\"invoice\"])\n",
        "    model_id = get_parameter(defaults[\"parameter_name\"], get_parameter(\"invoiceModelId\", \"prebuilt-invoice\"))\n",
        "    source_root = f\"lakehouse://{config['lakehouse']}/Files/documents/{defaults['source_subfolder']}\"\n",
        "    output_root = f\"lakehouse://{config['lakehouse']}/Files/raw/document-intelligence/{defaults['source_subfolder']}\"\n",
        "    processed_root = f\"lakehouse://{config['lakehouse']}/Files/raw/document-intelligence/_processed/{defaults['source_subfolder']}\"\n",
        "    doc_type_configs[doc_type] = {\n",
        "        \"model_id\": model_id,\n",
        "        \"source_root\": source_root,\n",
        "        \"output_root\": output_root,\n",
        "        \"processed_root\": processed_root,\n",
        "        \"header_table\": defaults[\"header_table\"],\n",
        "        \"line_table\": defaults[\"line_table\"]\n",
        "    }\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "print(json.dumps({\"documentTypes\": document_types, **config}, indent=2))\n",
        "print(\"Document type mapping:\")\n",
        "print(json.dumps(doc_type_configs, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "def ensure_directory(path: str) -> None:\n",
        "    directory = path.rsplit('/', 1)[0]\n",
        "    if directory:\n",
        "        try:\n",
        "            mssparkutils.fs.mkdirs(directory)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "def file_exists(path: str) -> bool:\n",
        "    try:\n",
        "        return bool(mssparkutils.fs.exists(path))\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def read_binary(path: str) -> bytes:\n",
        "    with mssparkutils.fs.open(path, 'rb') as handle:\n",
        "        return handle.read()\n",
        "\n",
        "\n",
        "def write_json(path: str, payload: Dict[str, Any]) -> None:\n",
        "    ensure_directory(path)\n",
        "    mssparkutils.fs.put(path, json.dumps(payload, indent=2), True)\n",
        "\n",
        "\n",
        "def list_files_recursive(root: str) -> List[str]:\n",
        "    items: List[str] = []\n",
        "    try:\n",
        "        entries = mssparkutils.fs.ls(root)\n",
        "    except Exception:\n",
        "        return items\n",
        "    for entry in entries:\n",
        "        if getattr(entry, 'isDir', False):\n",
        "            items.extend(list_files_recursive(entry.path))\n",
        "        else:\n",
        "            items.append(entry.path)\n",
        "    return items\n",
        "\n",
        "\n",
        "def guess_content_type(path: str) -> str:\n",
        "    lower = path.lower()\n",
        "    if lower.endswith('.pdf'):\n",
        "        return 'application/pdf'\n",
        "    if lower.endswith('.png'):\n",
        "        return 'image/png'\n",
        "    if lower.endswith(('.jpg', '.jpeg')):\n",
        "        return 'image/jpeg'\n",
        "    return 'application/octet-stream'\n",
        "\n",
        "\n",
        "def get_cognitive_token() -> str:\n",
        "    token = mssparkutils.credentials.getToken('https://cognitiveservices.azure.com/.default')\n",
        "    if isinstance(token, dict):\n",
        "        return token.get('token') or token.get('accessToken') or ''\n",
        "    if hasattr(token, 'token'):\n",
        "        return getattr(token, 'token')\n",
        "    return str(token)\n",
        "\n",
        "\n",
        "def analyze_document(content: bytes, content_type: str, model_id: str) -> Dict[str, Any]:\n",
        "    endpoint = config['endpoint'].rstrip('/')\n",
        "    url = f\"{endpoint}/formrecognizer/documentModels/{model_id}:analyze?api-version={config['api_version']}&stringIndexType=unicodeCodePoint\"\n",
        "    auth_header = {\"Authorization\": f\"Bearer {get_cognitive_token()}\", \"Content-Type\": content_type}\n",
        "    response = requests.post(url, headers=auth_header, data=content)\n",
        "    if response.status_code == 202:\n",
        "        operation_url = response.headers.get('operation-location') or response.headers.get('Operation-Location')\n",
        "        if not operation_url:\n",
        "            raise RuntimeError('Document Intelligence did not return an operation URL to poll.')\n",
        "        while True:\n",
        "            poll_response = requests.get(operation_url, headers={\"Authorization\": auth_header[\"Authorization\"]})\n",
        "            poll_response.raise_for_status()\n",
        "            payload = poll_response.json()\n",
        "            status = payload.get('status', '').lower()\n",
        "            if status == 'succeeded':\n",
        "                analyze_result = payload.get('analyzeResult', {})\n",
        "                return {\n",
        "                    \"documents\": analyze_result.get('documents', []),\n",
        "                    \"pages\": analyze_result.get('pages', []),\n",
        "                    \"modelId\": analyze_result.get('modelId') or payload.get('modelId'),\n",
        "                    \"modelVersion\": analyze_result.get('modelVersion'),\n",
        "                    \"apiVersion\": payload.get('apiVersion', config['api_version']),\n",
        "                    \"createdDateTime\": payload.get('createdDateTime'),\n",
        "                    \"lastUpdatedDateTime\": payload.get('lastUpdatedDateTime')\n",
        "                }\n",
        "            if status == 'failed':\n",
        "                raise RuntimeError(f\"Document Intelligence analysis failed: {json.dumps(payload)}\")\n",
        "            time.sleep(2)\n",
        "\n",
        "    response.raise_for_status()\n",
        "    payload = response.json()\n",
        "    analyze_result = payload.get('analyzeResult', payload)\n",
        "    return {\n",
        "        \"documents\": analyze_result.get('documents', []),\n",
        "        \"pages\": analyze_result.get('pages', []),\n",
        "        \"modelId\": analyze_result.get('modelId') or payload.get('modelId'),\n",
        "        \"modelVersion\": analyze_result.get('modelVersion'),\n",
        "        \"apiVersion\": payload.get('apiVersion', config['api_version']),\n",
        "        \"createdDateTime\": payload.get('createdDateTime'),\n",
        "        \"lastUpdatedDateTime\": payload.get('lastUpdatedDateTime')\n",
        "    }\n",
        "\n",
        "\n",
        "def field_value(field: Optional[Dict[str, Any]]) -> Any:\n",
        "    if not field:\n",
        "        return None\n",
        "    for key in ('valueString', 'valueDate', 'valueTime', 'valueInteger', 'valueNumber', 'valueCurrency'):\n",
        "        if key in field and field[key] is not None:\n",
        "            value = field[key]\n",
        "            if isinstance(value, dict):\n",
        "                return value.get('amount')\n",
        "            return value\n",
        "    if 'content' in field:\n",
        "        return field['content']\n",
        "    return None\n",
        "\n",
        "\n",
        "def field_confidence(field: Optional[Dict[str, Any]]) -> Optional[float]:\n",
        "    if not field:\n",
        "        return None\n",
        "    return field.get('confidence')\n",
        "\n",
        "\n",
        "def normalize_analysis(doc_type: str, file_name: str, source_path: str, analysis: Dict[str, Any]) -> Tuple[Dict[str, Any], List[Dict[str, Any]], Dict[str, Any]]:\n",
        "    documents = analysis.get('documents', [])\n",
        "    if not documents:\n",
        "        raise RuntimeError('Document Intelligence returned no documents to normalize.')\n",
        "\n",
        "    document = documents[0]\n",
        "    fields = document.get('fields', {})\n",
        "\n",
        "    header_row = {\n",
        "        'document_type': doc_type,\n",
        "        'document_id': document.get('docId', str(uuid.uuid4())),\n",
        "        'source_file_name': file_name,\n",
        "        'source_file_path': source_path,\n",
        "        'invoice_id': field_value(fields.get('InvoiceId')),\n",
        "        'purchase_order': field_value(fields.get('PurchaseOrder')),\n",
        "        'invoice_date': field_value(fields.get('InvoiceDate')),\n",
        "        'due_date': field_value(fields.get('DueDate')),\n",
        "        'total_amount': field_value(fields.get('Total')),\n",
        "        'subtotal_amount': field_value(fields.get('Subtotal')),\n",
        "        'tax_amount': field_value(fields.get('Tax')),\n",
        "        'amount_due': field_value(fields.get('AmountDue')),\n",
        "        'currency': field_value(fields.get('Currency')),\n",
        "        'currency_code': field_value(fields.get('CurrencyCode')),\n",
        "        'customer_name': field_value(fields.get('CustomerName')),\n",
        "        'customer_id': field_value(fields.get('CustomerId')),\n",
        "        'vendor_name': field_value(fields.get('VendorName')),\n",
        "        'vendor_address': field_value(fields.get('VendorAddress')),\n",
        "        'billing_address': field_value(fields.get('BillingAddress')),\n",
        "        'service_address': field_value(fields.get('ServiceAddress')),\n",
        "        'billing_recipient': field_value(fields.get('BillingAddressRecipient')),\n",
        "        'service_recipient': field_value(fields.get('ServiceAddressRecipient')),\n",
        "        'remittance_address': field_value(fields.get('RemittanceAddress')),\n",
        "        'remittance_recipient': field_value(fields.get('RemittanceAddressRecipient')),\n",
        "        'confidence': document.get('confidence'),\n",
        "        'model_id': analysis.get('modelId'),\n",
        "        'model_version': analysis.get('modelVersion'),\n",
        "        'api_version': analysis.get('apiVersion'),\n",
        "        'analyzed_at': datetime.utcnow().isoformat()\n",
        "    }\n",
        "\n",
        "    items_field = fields.get('Items') or fields.get('LineItems')\n",
        "    line_rows: List[Dict[str, Any]] = []\n",
        "    if items_field and 'valueArray' in items_field:\n",
        "        for index, item_entry in enumerate(items_field['valueArray'], start=1):\n",
        "            item_obj = item_entry.get('valueObject', {})\n",
        "            line_rows.append({\n",
        "                'document_type': doc_type,\n",
        "                'document_id': header_row['document_id'],\n",
        "                'line_number': index,\n",
        "                'description': field_value(item_obj.get('Description')),\n",
        "                'product_code': field_value(item_obj.get('ProductCode')),\n",
        "                'quantity': field_value(item_obj.get('Quantity')),\n",
        "                'unit': field_value(item_obj.get('Unit')),\n",
        "                'unit_price': field_value(item_obj.get('UnitPrice')),\n",
        "                'line_amount': field_value(item_obj.get('Amount')),\n",
        "                'line_tax': field_value(item_obj.get('Tax')),\n",
        "                'line_date': field_value(item_obj.get('Date')),\n",
        "                'line_currency': field_value(item_obj.get('Currency')),\n",
        "                'confidence': field_confidence(item_obj.get('Amount')) or item_obj.get('confidence')\n",
        "            })\n",
        "\n",
        "    manifest_info = {\n",
        "        'modelId': analysis.get('modelId'),\n",
        "        'apiVersion': analysis.get('apiVersion'),\n",
        "        'documentId': header_row['document_id'],\n",
        "        'confidence': document.get('confidence'),\n",
        "        'lineItemCount': len(line_rows)\n",
        "    }\n",
        "\n",
        "    return header_row, line_rows, manifest_info\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "header_tables: Dict[str, List[Dict[str, Any]]] = defaultdict(list)\n",
        "line_tables: Dict[str, List[Dict[str, Any]]] = defaultdict(list)\n",
        "summary: List[Dict[str, Any]] = []\n",
        "\n",
        "for doc_type, type_config in doc_type_configs.items():\n",
        "    files = [path for path in list_files_recursive(type_config['source_root']) if path.lower().endswith(('.pdf', '.png', '.jpg', '.jpeg'))]\n",
        "    files.sort()\n",
        "    processed = 0\n",
        "\n",
        "    for file_path in files:\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        output_path = f\"{type_config['output_root']}/{file_name}.json\"\n",
        "        manifest_path = f\"{type_config['processed_root']}/{file_name}.json\"\n",
        "\n",
        "        if not config['force_reprocess'] and file_exists(output_path):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            content_bytes = read_binary(file_path)\n",
        "            analysis = analyze_document(content_bytes, guess_content_type(file_name), type_config['model_id'])\n",
        "            header_row, line_rows, manifest_info = normalize_analysis(doc_type, file_name, file_path, analysis)\n",
        "\n",
        "            normalized_payload = {\n",
        "                'header': header_row,\n",
        "                'lineItems': line_rows,\n",
        "                'modelId': manifest_info['modelId'],\n",
        "                'apiVersion': manifest_info['apiVersion'],\n",
        "                'sourcePath': file_path\n",
        "            }\n",
        "            write_json(output_path, normalized_payload)\n",
        "\n",
        "            manifest_payload = {\n",
        "                'documentType': doc_type,\n",
        "                'sourcePath': file_path,\n",
        "                'outputPath': output_path,\n",
        "                'processedAt': datetime.utcnow().isoformat(),\n",
        "                'modelId': manifest_info['modelId'],\n",
        "                'apiVersion': manifest_info['apiVersion'],\n",
        "                'header': header_row,\n",
        "                'lineItemCount': manifest_info['lineItemCount']\n",
        "            }\n",
        "            write_json(manifest_path, manifest_payload)\n",
        "\n",
        "            header_tables[type_config['header_table']].append(header_row)\n",
        "            if line_rows:\n",
        "                line_tables[type_config['line_table']].extend(line_rows)\n",
        "\n",
        "            processed += 1\n",
        "            if processed >= config['max_documents']:\n",
        "                break\n",
        "\n",
        "        except Exception as err:\n",
        "            print(f\"[WARN] Failed to process {file_path}: {err}\")\n",
        "\n",
        "    summary.append({\n",
        "        'document_type': doc_type,\n",
        "        'processed': processed,\n",
        "        'total_detected': len(files)\n",
        "    })\n",
        "\n",
        "for table_name, rows in header_tables.items():\n",
        "    if rows:\n",
        "        df = spark.createDataFrame(rows)\n",
        "        df = df.select([F.col(col).alias(col) for col in df.columns])\n",
        "        df.write.mode('overwrite').saveAsTable(table_name)\n",
        "\n",
        "for table_name, rows in line_tables.items():\n",
        "    if rows:\n",
        "        df = spark.createDataFrame(rows)\n",
        "        df = df.select([F.col(col).alias(col) for col in df.columns])\n",
        "        df.write.mode('overwrite').saveAsTable(table_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "summary_df = (\n",
        "    spark.createDataFrame(summary)\n",
        "    if summary\n",
        "    else spark.createDataFrame([], 'document_type string, processed int, total_detected int')\n",
        ")\n",
        "\n",
        "display(summary_df)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "fabric": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}